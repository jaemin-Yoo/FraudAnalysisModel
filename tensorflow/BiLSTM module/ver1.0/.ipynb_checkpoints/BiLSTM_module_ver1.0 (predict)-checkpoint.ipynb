{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b5dc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from konlpy.tag import Okt\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "file_name = \"BiLSTM_module_ver1.0\"\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "\n",
    "with open(file_name+'_variable.json') as f:\n",
    "    var = json.load(f)\n",
    "\n",
    "max_data = var['max_data'] # 데이터 문자열 최대 길이\n",
    "min_data = var['min_data'] # 데이터 문자열 최소 길이\n",
    "max_len = var['max_len'] # 전체 데이터 셋 길이 설정 (메일의 최대 길이)\n",
    "\n",
    "with open(file_name+'_tokenizer.pickle', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "model = tf.keras.models.load_model(file_name+'.h5')\n",
    "model.summary()\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "def preTreatment(data):\n",
    "    stopwords = ['\\n','.','?',',','']\n",
    "    temp = okt.morphs(data, stem=True) # 토큰화\n",
    "    temp = [word for word in temp if not word in stopwords] # 불용어 처리\n",
    "    return temp\n",
    "\n",
    "\n",
    "def sentiment_predict(new_sentence):\n",
    "    data = tokenizer.texts_to_sequences([new_sentence]) # 단어를 숫자값, 인덱스로 변환하여 저장\n",
    "    print(data)\n",
    "    pad_new = pad_sequences(data, maxlen = max_len) # 모든 메일의 길이를 100로 설정 (빈 부분은 0으로 패딩)\n",
    "    score = float(model.predict(pad_new))\n",
    "    if (score > 0.5):\n",
    "        print(' {:.2f}% 확률로 스팸입니다.'.format(score * 100))\n",
    "    else:\n",
    "        print(' {:.2f}% 확률로 스팸이 아닙니다.'.format((1 - score) * 100))\n",
    "    \n",
    "    print('-------------------------------------------------------------------------')\n",
    "        \n",
    "\n",
    "# 음성파일 STT 변환\n",
    "import speech_recognition as sr\n",
    "\n",
    "r = sr.Recognizer()\n",
    "harvard = sr.AudioFile('ham1.wav') # 100MB 용량 제한\n",
    "with harvard as source:\n",
    "    audio = r.record(source)\n",
    "\n",
    "t = r.recognize_google(audio, language='ko-KR')\n",
    "\n",
    "temp_X = []\n",
    "x = [t[i:i+max_data] for i in range(0, len(t), max_data)] # 텍스트 파일 중 150 길이로 데이터 길이 제한\n",
    "for s in x:\n",
    "    if len(s) > min_data: # 길이가 50 이하인 데이터 삭제\n",
    "        temp_X.append(s)\n",
    "x = temp_X\n",
    "for txt in x:\n",
    "    print(txt)\n",
    "    data = preTreatment(txt)\n",
    "    sentiment_predict(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
