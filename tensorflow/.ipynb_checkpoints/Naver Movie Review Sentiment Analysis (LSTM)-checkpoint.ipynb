{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\n",
    "\n",
    "train_data = pd.read_table('ratings_train.txt')\n",
    "test_data = pd.read_table('ratings_test.txt')\n",
    "\n",
    "train_data['document'].nunique(), train_data['label'].nunique() # 데이터 중복 있는지 확인\n",
    "train_data.drop_duplicates(subset=['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "train_data.loc[train_data.document.isnull()]\n",
    "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "# 한글과 공백을 제외하고 모두 제거\n",
    "train_data['document'] = train_data['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n",
    "train_data['document'].replace('', np.nan, inplace=True)\n",
    "train_data = train_data.dropna(how = 'any')\n",
    "\n",
    "test_data.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
    "test_data['document'] = test_data['document'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n",
    "test_data['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
    "test_data = test_data.dropna(how='any') # Null 값 제거\n",
    "\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "X_train = []\n",
    "for sentence in train_data['document']:\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(temp_X)\n",
    "    \n",
    "X_test = []\n",
    "for sentence in test_data['document']:\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_test.append(temp_X)\n",
    "\n",
    "print('test1')\n",
    "    \n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(X_train)\n",
    "# threshold = 3\n",
    "# total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "# rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "# total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "# rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# # 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "# for key, value in tokenizer.word_counts.items():\n",
    "#     total_freq = total_freq + value\n",
    "\n",
    "#     # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "#     if(value < threshold):\n",
    "#         rare_cnt = rare_cnt + 1\n",
    "#         rare_freq = rare_freq + value\n",
    "        \n",
    "# vocab_size = total_cnt - rare_cnt + 1\n",
    "# tokenizer = Tokenizer(vocab_size) \n",
    "# tokenizer.fit_on_texts(X_train)\n",
    "# X_train = tokenizer.texts_to_sequences(X_train)\n",
    "# X_test = tokenizer.texts_to_sequences(X_test)\n",
    "# y_train = np.array(train_data['label'])\n",
    "# y_test = np.array(test_data['label'])\n",
    "# drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]\n",
    "\n",
    "# print('test')\n",
    "\n",
    "# # 빈 샘플들을 제거\n",
    "# X_train = np.delete(X_train, drop_train, axis=0)\n",
    "# y_train = np.delete(y_train, drop_train, axis=0)\n",
    "\n",
    "# print('리뷰의 최대 길이 :',max(len(l) for l in X_train))\n",
    "# print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))\n",
    "# plt.hist([len(s) for s in X_train], bins=50)\n",
    "# plt.xlabel('length of samples')\n",
    "# plt.ylabel('number of samples')\n",
    "# plt.show()\n",
    "\n",
    "# def below_threshold_len(max_len, nested_list):\n",
    "#   cnt = 0\n",
    "#   for s in nested_list:\n",
    "#     if(len(s) <= max_len):\n",
    "#         cnt = cnt + 1\n",
    "#   print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))\n",
    "\n",
    "# max_len = 30\n",
    "# below_threshold_len(max_len, X_train)\n",
    "\n",
    "# X_train = pad_sequences(X_train, maxlen = max_len)\n",
    "# X_test = pad_sequences(X_test, maxlen = max_len)\n",
    "\n",
    "# from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(vocab_size, 100))\n",
    "# model.add(LSTM(128))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "# mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "# model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "# history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)\n",
    "\n",
    "# loaded_model = load_model('best_model.h5')\n",
    "# print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))\n",
    "\n",
    "# def sentiment_predict(new_sentence):\n",
    "#   new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
    "#   new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
    "#   encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
    "#   pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
    "#   score = float(loaded_model.predict(pad_new)) # 예측\n",
    "#   if(score > 0.5):\n",
    "#     print(\"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(score * 100))\n",
    "#   else:\n",
    "#     print(\"{:.2f}% 확률로 부정 리뷰입니다.\\n\".format((1 - score) * 100))\n",
    "\n",
    "# print(sentiment_predict('이 영화 개꿀잼 ㅋㅋㅋ'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
