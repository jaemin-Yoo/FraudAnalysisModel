{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a281377e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "스팸 데이터 개수 : 76(read) -> 1304(spilt) -> 1279(drop)\n",
      "스팸 데이터 최소 길이 53\n",
      "스팸 데이터 최대 길이 150\n",
      "----------------------------------------------------\n",
      "일반 데이터 개수 : 8000(read) -> 10398(spilt) -> 8896(drop)\n",
      "일반 데이터 최소 길이 50\n",
      "일반 데이터 최대 길이 150\n",
      "----------------------------------------------------\n",
      "데이터 총 개수 : 10175\n",
      "훈련용 데이터 개수 : 7631\n",
      "테스트용 데이터 개수 : 2544\n",
      "----------------------------------------------------\n",
      "   label  count\n",
      "0      0   6688\n",
      "1      1    943\n",
      "----------------------------------------------------\n",
      "스팸 단어 빈도\n",
      "[('나', 3054), ('도', 2274), ('그렇다', 2268), ('안', 2140), ('아니다', 1951), ('먹다', 1932), ('아', 1909), ('내', 1804), ('보다', 1783), ('있다', 1573), ('은', 1526), ('같다', 1443), ('가다', 1401), ('다', 1356), ('진짜', 1330), ('근데', 1217), ('자다', 1179), ('오다', 1164), ('너', 1156), ('그', 1151)]\n",
      "----------------------------------------------------\n",
      "일상 단어 빈도\n",
      "[('네', 1118), ('을', 1004), ('본인', 865), ('있다', 820), ('되다', 796), ('를', 659), ('지금', 579), ('거', 551), ('으로', 429), ('저희', 422), ('예', 413), ('제', 412), ('아니다', 379), ('요', 357), ('로', 352), ('은', 345), ('통장', 342), ('그렇다', 337), ('에서', 312), ('받다', 305)]\n",
      "----------------------------------------------------\n",
      "단어 집합(vocabulary)의 크기 : 19014\n",
      "등장 빈도가 1번 이하인 희귀 단어의 수: 9688\n",
      "단어 집합에서 희귀 단어의 비율: 50.95193015672662\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 3.6212355111332064\n",
      "빈 샘플 개수 : 1\n",
      "최대 길이 : 65\n",
      "평균 길이 : 35.06330275229358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YH\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQeklEQVR4nO3ccahed33H8ffHRLtOV2zpTYj3JmvHoi4tqOslZghjM9ua4TD9pxBha5DCHSUOhcFM9s/YH4Hun7EV1kJQ15Q5Q3CTBl3dsmxljGXGW+2Mac1yZ2t7l6yJOrHuj2rid3/cn/hw8+Te57bJcyW/9wsO55zv+f3O8zuQfJ7D7znnpqqQJPXhdas9AEnS+Bj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdWTb0k7wtydMDy3eTfCTJLUmOJjnT1jcP9NmXZC7J6SR3D9TvSnKyHXsoSa7VhUmSLpeVPKefZA3w38C7gT3At6vqwSR7gZur6qNJtgCfArYCbwH+EXhrVV1KcgL4MPDvwN8BD1XVE1f1iiRJV7TS6Z3twH9V1TeAncDBVj8I3NO2dwKHquqVqnoOmAO2JtkA3FRVx2vhm+axgT6SpDFYu8L2u1i4iwdYX1XnAKrqXJJ1rT7Jwp38j8y32g/a9uL6km699da67bbbVjhMSerbU0899c2qmlhcHzn0k7wBeD+wb7mmQ2q1RH3YZ80AMwCbNm1idnZ21GFKkoAk3xhWX8n0zm8CX6qql9r+S23KhrY+3+rzwMaBflPA2VafGlK/TFUdqKrpqpqemLjsi0qS9CqtJPQ/wI+ndgCOALvb9m7g8YH6riQ3JLkd2AycaFNBLyfZ1p7auW+gjyRpDEaa3kny08CvA787UH4QOJzkfuAF4F6AqjqV5DDwDHAR2FNVl1qfB4BHgRuBJ9oiSRqTFT2yuRqmp6fLOX1JWpkkT1XV9OK6b+RKUkcMfUnqiKEvSR0x9CWpIyt9I1dXcNvez632EK4bzz/4vtUegnTd8k5fkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOjJS6Cd5c5JPJ/lakmeT/FKSW5IcTXKmrW8eaL8vyVyS00nuHqjfleRkO/ZQklyLi5IkDTfqnf6fA5+vqrcD7wCeBfYCx6pqM3Cs7ZNkC7ALuAPYATycZE07zyPADLC5LTuu0nVIkkawbOgnuQn4ZeDjAFX1/ar6DrATONiaHQTuads7gUNV9UpVPQfMAVuTbABuqqrjVVXAYwN9JEljMMqd/s8BF4C/TPLlJB9L8kZgfVWdA2jrda39JPDiQP/5Vpts24vrkqQxGSX01wK/CDxSVe8C/o82lXMFw+bpa4n65SdIZpLMJpm9cOHCCEOUJI1ilNCfB+ar6gtt/9MsfAm81KZsaOvzA+03DvSfAs62+tSQ+mWq6kBVTVfV9MTExKjXIklaxrKhX1X/A7yY5G2ttB14BjgC7G613cDjbfsIsCvJDUluZ+EH2xNtCujlJNvaUzv3DfSRJI3B2hHb/R7wySRvAL4OfJCFL4zDSe4HXgDuBaiqU0kOs/DFcBHYU1WX2nkeAB4FbgSeaIskaUxGCv2qehqYHnJo+xXa7wf2D6nPAneuYHySpKvIN3IlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHRgr9JM8nOZnk6SSzrXZLkqNJzrT1zQPt9yWZS3I6yd0D9bvaeeaSPJQkV/+SJElXspI7/V+tqndW1XTb3wscq6rNwLG2T5ItwC7gDmAH8HCSNa3PI8AMsLktO177JUiSRvVapnd2Agfb9kHgnoH6oap6paqeA+aArUk2ADdV1fGqKuCxgT6SpDEYNfQL+IckTyWZabX1VXUOoK3Xtfok8OJA3/lWm2zbi+uSpDFZO2K791TV2STrgKNJvrZE22Hz9LVE/fITLHyxzABs2rRpxCFKkpYz0p1+VZ1t6/PAZ4CtwEttyoa2Pt+azwMbB7pPAWdbfWpIfdjnHaiq6aqanpiYGP1qJElLWjb0k7wxyc/8aBv4DeCrwBFgd2u2G3i8bR8BdiW5IcntLPxge6JNAb2cZFt7aue+gT6SpDEYZXpnPfCZ9nTlWuCvq+rzSb4IHE5yP/ACcC9AVZ1Kchh4BrgI7KmqS+1cDwCPAjcCT7RFkjQmy4Z+VX0deMeQ+reA7Vfosx/YP6Q+C9y58mFKkq4G38iVpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdGTn0k6xJ8uUkn237tyQ5muRMW9880HZfkrkkp5PcPVC/K8nJduyhJLm6lyNJWspK7vQ/DDw7sL8XOFZVm4FjbZ8kW4BdwB3ADuDhJGtan0eAGWBzW3a8ptFLklZkpNBPMgW8D/jYQHkncLBtHwTuGagfqqpXquo5YA7YmmQDcFNVHa+qAh4b6CNJGoNR7/T/DPgD4IcDtfVVdQ6grde1+iTw4kC7+VabbNuL65KkMVk29JP8FnC+qp4a8ZzD5ulrifqwz5xJMptk9sKFCyN+rCRpOaPc6b8HeH+S54FDwHuT/BXwUpuyoa3Pt/bzwMaB/lPA2VafGlK/TFUdqKrpqpqemJhYweVIkpaybOhX1b6qmqqq21j4gfafquq3gSPA7tZsN/B42z4C7EpyQ5LbWfjB9kSbAno5ybb21M59A30kSWOw9jX0fRA4nOR+4AXgXoCqOpXkMPAMcBHYU1WXWp8HgEeBG4En2iJJGpMVhX5VPQk82ba/BWy/Qrv9wP4h9VngzpUOUpJ0dfhGriR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6siyoZ/kp5KcSPIfSU4l+eNWvyXJ0SRn2vrmgT77kswlOZ3k7oH6XUlOtmMPJcm1uSxJ0jCj3Om/Ary3qt4BvBPYkWQbsBc4VlWbgWNtnyRbgF3AHcAO4OEka9q5HgFmgM1t2XH1LkWStJxlQ78WfK/tvr4tBewEDrb6QeCetr0TOFRVr1TVc8AcsDXJBuCmqjpeVQU8NtBHkjQGI83pJ1mT5GngPHC0qr4ArK+qcwBtva41nwReHOg+32qTbXtxXZI0JiOFflVdqqp3AlMs3LXfuUTzYfP0tUT98hMkM0lmk8xeuHBhlCFKkkawoqd3quo7wJMszMW/1KZsaOvzrdk8sHGg2xRwttWnhtSHfc6BqpququmJiYmVDFGStIRRnt6ZSPLmtn0j8GvA14AjwO7WbDfweNs+AuxKckOS21n4wfZEmwJ6Ocm29tTOfQN9JEljsHaENhuAg+0JnNcBh6vqs0mOA4eT3A+8ANwLUFWnkhwGngEuAnuq6lI71wPAo8CNwBNtkSSNybKhX1VfAd41pP4tYPsV+uwH9g+pzwJL/R4gSbqGfCNXkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1ZNnQT7IxyT8neTbJqSQfbvVbkhxNcqatbx7osy/JXJLTSe4eqN+V5GQ79lCSXJvLkiQNM8qd/kXg96vqF4BtwJ4kW4C9wLGq2gwca/u0Y7uAO4AdwMNJ1rRzPQLMAJvbsuMqXoskaRnLhn5VnauqL7Xtl4FngUlgJ3CwNTsI3NO2dwKHquqVqnoOmAO2JtkA3FRVx6uqgMcG+kiSxmBFc/pJbgPeBXwBWF9V52DhiwFY15pNAi8OdJtvtcm2vbg+7HNmkswmmb1w4cJKhihJWsLIoZ/kTcDfAB+pqu8u1XRIrZaoX16sOlBV01U1PTExMeoQJUnLGCn0k7yehcD/ZFX9bSu/1KZsaOvzrT4PbBzoPgWcbfWpIXVJ0piM8vROgI8Dz1bVnw4cOgLsbtu7gccH6ruS3JDkdhZ+sD3RpoBeTrKtnfO+gT6SpDFYO0Kb9wC/A5xM8nSr/SHwIHA4yf3AC8C9AFV1Kslh4BkWnvzZU1WXWr8HgEeBG4En2iJJGpNlQ7+q/pXh8/EA26/QZz+wf0h9FrhzJQOUJF09vpErSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6smzoJ/lEkvNJvjpQuyXJ0SRn2vrmgWP7kswlOZ3k7oH6XUlOtmMPJcnVvxxJ0lJGudN/FNixqLYXOFZVm4FjbZ8kW4BdwB2tz8NJ1rQ+jwAzwOa2LD6nJOkaWzb0q+pfgG8vKu8EDrbtg8A9A/VDVfVKVT0HzAFbk2wAbqqq41VVwGMDfSRJY/Jq5/TXV9U5gLZe1+qTwIsD7eZbbbJtL64PlWQmyWyS2QsXLrzKIUqSFrvaP+QOm6evJepDVdWBqpququmJiYmrNjhJ6t3aV9nvpSQbqupcm7o53+rzwMaBdlPA2VafGlKXdI3dtvdzqz2E68rzD75vtYfwmrzaO/0jwO62vRt4fKC+K8kNSW5n4QfbE20K6OUk29pTO/cN9JEkjcmyd/pJPgX8CnBrknngj4AHgcNJ7gdeAO4FqKpTSQ4DzwAXgT1Vdamd6gEWngS6EXiiLZKkMVo29KvqA1c4tP0K7fcD+4fUZ4E7VzQ6SdJV5Ru5ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI2MP/SQ7kpxOMpdk77g/X5J6NtbQT7IG+AvgN4EtwAeSbBnnGCSpZ+O+098KzFXV16vq+8AhYOeYxyBJ3Vo75s+bBF4c2J8H3r24UZIZYKbtfi/J6TGMrQe3At9c7UEsJ3+y2iPQKvHf59X1s8OK4w79DKnVZYWqA8CBaz+cviSZrarp1R6HNIz/Psdj3NM788DGgf0p4OyYxyBJ3Rp36H8R2Jzk9iRvAHYBR8Y8Bknq1lind6rqYpIPAX8PrAE+UVWnxjmGzjllpp9k/vscg1RdNqUuSbpO+UauJHXE0Jekjhj6ktSRcT+nrzFK8nYW3nieZOF9iLPAkap6dlUHJmnVeKd/nUryURb+zEWAEyw8LhvgU/6hO/0kS/LB1R7D9cynd65TSf4TuKOqfrCo/gbgVFVtXp2RSUtL8kJVbVrtcVyvnN65fv0QeAvwjUX1De2YtGqSfOVKh4D14xxLbwz969dHgGNJzvDjP3K3Cfh54EOrNSipWQ/cDfzvonqAfxv/cPph6F+nqurzSd7Kwp+znmThP9M88MWqurSqg5Pgs8CbqurpxQeSPDn20XTEOX1J6ohP70hSRwx9SeqIoS9JHTH0Jakjhr4kdeT/AX1DdXbueJ3TAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "file_name = \"LSTM_module_ver3.0\"\n",
    "\n",
    "spam_temp = []\n",
    "ham_temp = []\n",
    "spam_X = []\n",
    "ham_X = []\n",
    "train_X = []\n",
    "test_X = []\n",
    "\n",
    "ham_cnt = 8000 # ham Data Count\n",
    "max_data = 150\n",
    "min_data = 50\n",
    "\n",
    "# 피싱 대화 데이터 가져오기\n",
    "path = 'fraudDataset/spamData/'\n",
    "path1 = os.listdir(path)\n",
    "for a in path1:\n",
    "    path2 = path+a\n",
    "    with open(path2, 'r', encoding='utf-8') as f:\n",
    "        contents = f.read()\n",
    "        spam_temp.append(contents)\n",
    "                \n",
    "for t in spam_temp:\n",
    "    x = [t[i:i+max_data] for i in range(0, len(t), max_data)] # 텍스트 파일 중 150 길이로 데이터 길이 제한\n",
    "    spam_X.extend(x)\n",
    "    \n",
    "\n",
    "spam_df = pd.DataFrame(spam_X, columns=['text'])\n",
    "spam_df.drop(spam_df[spam_df['text'].str.len() < min_data].index, inplace=True) # 길이가 min_data 이하인 데이터 삭제\n",
    "# spam_df['text'].nunique() # 중복 확인\n",
    "spam_df.drop_duplicates(subset=['text'], inplace=True) # 중복 제거\n",
    "spam_df['label'] = 1\n",
    "\n",
    "print(\"스팸 데이터 개수 : {}(read) -> {}(spilt) -> {}(drop)\".format(len(spam_temp),len(spam_X),len(spam_df)))\n",
    "print('스팸 데이터 최소 길이',len(min(spam_df['text'], key=len)))\n",
    "print('스팸 데이터 최대 길이',len(max(spam_df['text'], key=len)))\n",
    "print('----------------------------------------------------')\n",
    "\n",
    "# 일상 대화 데이터 가져오기\n",
    "path = 'fraudDataset/hamData/'\n",
    "path1 = os.listdir(path)\n",
    "for a in path1:\n",
    "    path2 = os.listdir(path+a)\n",
    "    for b in path2:\n",
    "        path3 = path+a+'/'+b\n",
    "        with open(path3, 'r', encoding='utf-8') as f:\n",
    "            contents = f.read()\n",
    "            json_data = json.loads(contents)\n",
    "            data = ''\n",
    "            data = json_data['data']\n",
    "            for i in range(len(data)):                \n",
    "                sentence = ''\n",
    "                dialogue = data[i]['body']['dialogue']\n",
    "                for j in range(len(dialogue)):\n",
    "                    utterance = dialogue[j]['utterance']\n",
    "                    sentence += utterance+' '\n",
    "                    \n",
    "                if len(ham_temp) == ham_cnt:\n",
    "                    break\n",
    "                    \n",
    "                ham_temp.append(sentence)\n",
    "                            \n",
    "for t in ham_temp:\n",
    "    x = [t[i:i+max_data] for i in range(0, len(t), max_data)] # 텍스트 파일 중 150 길이로 데이터 길이 제한\n",
    "    ham_X.extend(x)\n",
    "    \n",
    "ham_df = pd.DataFrame(ham_X, columns=['text'])\n",
    "ham_df.drop(ham_df[ham_df['text'].str.len() < min_data].index, inplace=True) # 길이가 min_data 이하인 데이터 삭제\n",
    "# ham_df['text'].nunique() # 중복 확인\n",
    "ham_df.drop_duplicates(subset=['text'], inplace=True) # 중복 제거\n",
    "ham_df['label'] = 0\n",
    "\n",
    "print(\"일반 데이터 개수 : {}(read) -> {}(spilt) -> {}(drop)\".format(len(ham_temp),len(ham_X),len(ham_df)))\n",
    "print('일반 데이터 최소 길이',len(min(ham_df['text'], key=len)))\n",
    "print('일반 데이터 최대 길이',len(max(ham_df['text'], key=len)))\n",
    "print('----------------------------------------------------')\n",
    "\n",
    "total_df = pd.concat([spam_df, ham_df], ignore_index=True)\n",
    "#print('Null 데이터 확인 : ',total_df.isnull().values.any())\n",
    "\n",
    "train_data, test_data = train_test_split(total_df, test_size = 0.25, random_state = 42) # 훈련용 테스트 데이터 나누기\n",
    "print('데이터 총 개수 :',len(total_df))\n",
    "print('훈련용 데이터 개수 :', len(train_data))\n",
    "print('테스트용 데이터 개수 :', len(test_data))\n",
    "print('----------------------------------------------------')\n",
    "\n",
    "plt.subplot(121)\n",
    "train_data['label'].value_counts().plot(kind = 'bar', title='Count')\n",
    "print(train_data.groupby('label').size().reset_index(name = 'count'))\n",
    "print('----------------------------------------------------')\n",
    "\n",
    "\n",
    "# 한글과 공백을 제외하고 모두 제거\n",
    "pd.set_option('mode.chained_assignment', None) # 경고 메시지 끄기\n",
    "train_data['text'] = train_data['text'].str.replace(\"#@(.+?)#|[^가-힣 ]\",\"\",regex=True)\n",
    "train_data.replace('', np.nan, inplace=True)\n",
    "#print(train_data.isnull().sum()) # null 데이터 확인\n",
    "test_data['text'] = test_data['text'].str.replace(\"#@(.+?)#|[^가-힣 ]\",\"\",regex=True)\n",
    "test_data.replace('', np.nan, inplace=True)\n",
    "#print(test_data.isnull().sum())\n",
    "\n",
    "okt = Okt()\n",
    "stopwords = ['하다','이','가','에','는', '없다']\n",
    "\n",
    "X_train = []\n",
    "for sentence in train_data['text']:\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 처리\n",
    "    X_train.append(temp_X)\n",
    "\n",
    "X_test = []\n",
    "for sentence in test_data['text']:\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 처리\n",
    "    X_test.append(temp_X)\n",
    "    \n",
    "    \n",
    "train_data['tokenized'] = X_train\n",
    "test_data['tokenized'] = X_test\n",
    "\n",
    "spam_words = np.hstack(train_data[train_data.label == 1]['tokenized'].values)\n",
    "ham_words = np.hstack(train_data[train_data.label == 0]['tokenized'].values)\n",
    "\n",
    "spam_word_count = Counter(spam_words)\n",
    "print('스팸 단어 빈도')\n",
    "print(spam_word_count.most_common(20))\n",
    "print('----------------------------------------------------')\n",
    "\n",
    "ham_word_count = Counter(ham_words)\n",
    "print('일상 단어 빈도')\n",
    "print(ham_word_count.most_common(20))\n",
    "print('----------------------------------------------------')\n",
    "\n",
    "X_train = train_data['tokenized'].values\n",
    "y_train = train_data['label'].values\n",
    "X_test = test_data['tokenized'].values\n",
    "y_test = test_data['label'].values\n",
    "\n",
    "# 한글은 형태소 분석기 사용해야됨 KoNPLY\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "threshold = 2 # 등장 횟수가 threshold인 단어는 제거\n",
    "word_to_index = tokenizer.word_index\n",
    "total_cnt = len(word_to_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 토큰화된 각 단어 index를 json 형태로 저장\n",
    "json_index = json.dumps(word_to_index)\n",
    "with open(file_name+\"_wordIndex.json\", \"w\") as f:\n",
    "    f.write(json_index)\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "        \n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n",
    "\n",
    "#vocab_size = total_cnt - rare_cnt + 1\n",
    "#vocab_size = total_cnt + 1 # predict Error 해결\n",
    "\n",
    "vocab_size = total_cnt - rare_cnt + 2\n",
    "tokenizer = Tokenizer(vocab_size, oov_token = 'OOV')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# 빈 샘플들을 제거\n",
    "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]\n",
    "print(\"빈 샘플 개수 :\",len(drop_train))\n",
    "X_train = np.delete(X_train, drop_train, axis=0)\n",
    "y_train = np.delete(y_train, drop_train, axis=0)\n",
    "\n",
    "print('최대 길이 :',max(len(l) for l in X_train))\n",
    "print('평균 길이 :',sum(map(len, X_train))/len(X_train))\n",
    "plt.subplot(122)\n",
    "plt.hist([len(s) for s in X_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "def below_threshold_len(max_len, nested_list):\n",
    "    cnt = 0\n",
    "    for s in nested_list:\n",
    "        if(len(s) <= max_len):\n",
    "            cnt = cnt + 1\n",
    "    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))\n",
    "\n",
    "max_len = 60 # 최대 길이 (그래프를 보고 판단)\n",
    "below_threshold_len(max_len, X_train)\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen = max_len)\n",
    "X_test = pad_sequences(X_test, maxlen = max_len)\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, max_len))\n",
    "model.add(Bidirectional(LSTM(100)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint(file_name+'.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=256, validation_split=0.2)\n",
    "\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))\n",
    "\n",
    "epochs = range(1, len(history.history['acc']) + 1)\n",
    "plt.plot(epochs, history.history['loss'])\n",
    "plt.plot(epochs, history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "# 변수 파일 저장\n",
    "d = {'max_data':max_data, 'min_data':min_data, 'max_len':max_len}\n",
    "json_var = json.dumps(d)\n",
    "with open(file_name+'_variable.json', 'w') as f:\n",
    "    f.write(json_var)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
