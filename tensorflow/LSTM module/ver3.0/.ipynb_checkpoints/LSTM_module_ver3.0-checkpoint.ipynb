{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a281377e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1279\n",
      "1281\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "file_name = \"LSTM_module_ver3.0\"\n",
    "\n",
    "spam_temp = []\n",
    "ham_temp = []\n",
    "spam_X = []\n",
    "ham_X = []\n",
    "train_X = []\n",
    "test_X = []\n",
    "\n",
    "ham_cnt = 10000 # ham Data Count\n",
    "max_data = 150\n",
    "min_data = 50\n",
    "\n",
    "# 피싱 대화 데이터 가져오기\n",
    "path = '../../fraudDataset/spamData/'\n",
    "path1 = os.listdir(path)\n",
    "for a in path1:\n",
    "    path2 = path+a\n",
    "    with open(path2, 'r', encoding='utf-8') as f:\n",
    "        contents = f.read()\n",
    "        spam_temp.append(contents)\n",
    "                \n",
    "for t in spam_temp:\n",
    "    x = [t[i:i+max_data] for i in range(0, len(t), max_data)] # 텍스트 파일 중 150 길이로 데이터 길이 제한\n",
    "    spam_X.extend(x)\n",
    "    \n",
    "\n",
    "spam_df = pd.DataFrame(spam_X, columns=['text'])\n",
    "spam_df.drop(spam_df[spam_df['text'].str.len() < min_data].index, inplace=True) # 길이가 min_data 미만인 데이터 삭제\n",
    "# spam_df['text'].nunique() # 중복 확인\n",
    "spam_df.drop_duplicates(subset=['text'], inplace=True) # 중복 제거\n",
    "spam_df['label'] = 1\n",
    "\n",
    "print(\"스팸 데이터 개수 : {}(read) -> {}(spilt) -> {}(drop)\".format(len(spam_temp),len(spam_X),len(spam_df)))\n",
    "print('스팸 데이터 최소 길이',len(min(spam_df['text'], key=len)))\n",
    "print('스팸 데이터 최대 길이',len(max(spam_df['text'], key=len)))\n",
    "print('----------------------------------------------------')\n",
    "\n",
    "# 일상 대화 데이터 가져오기\n",
    "path = 'fraudDataset/hamData/'\n",
    "path1 = os.listdir(path)\n",
    "for a in path1:\n",
    "    path2 = os.listdir(path+a)\n",
    "    for b in path2:\n",
    "        path3 = path+a+'/'+b\n",
    "        with open(path3, 'r', encoding='utf-8') as f:\n",
    "            contents = f.read()\n",
    "            json_data = json.loads(contents)\n",
    "            data = ''\n",
    "            data = json_data['data']\n",
    "            for i in range(len(data)):                \n",
    "                sentence = ''\n",
    "                dialogue = data[i]['body']['dialogue']\n",
    "                for j in range(len(dialogue)):\n",
    "                    utterance = dialogue[j]['utterance']\n",
    "                    sentence += utterance+' '\n",
    "                    \n",
    "                if len(ham_temp) == ham_cnt:\n",
    "                    break\n",
    "                    \n",
    "                ham_temp.append(sentence)\n",
    "                            \n",
    "for t in ham_temp:\n",
    "    x = [t[i:i+max_data] for i in range(0, len(t), max_data)] # 텍스트 파일 중 150 길이로 데이터 길이 제한\n",
    "    ham_X.extend(x)\n",
    "    \n",
    "ham_df = pd.DataFrame(ham_X, columns=['text'])\n",
    "ham_df.drop(ham_df[ham_df['text'].str.len() < min_data].index, inplace=True) # 길이가 min_data 미만인 데이터 삭제\n",
    "# ham_df['text'].nunique() # 중복 확인\n",
    "ham_df.drop_duplicates(subset=['text'], inplace=True) # 중복 제거\n",
    "ham_df['label'] = 0\n",
    "\n",
    "print(\"일반 데이터 개수 : {}(read) -> {}(spilt) -> {}(drop)\".format(len(ham_temp),len(ham_X),len(ham_df)))\n",
    "print('일반 데이터 최소 길이',len(min(ham_df['text'], key=len)))\n",
    "print('일반 데이터 최대 길이',len(max(ham_df['text'], key=len)))\n",
    "print('----------------------------------------------------')\n",
    "\n",
    "total_df = pd.concat([spam_df, ham_df], ignore_index=True)\n",
    "#print('Null 데이터 확인 : ',total_df.isnull().values.any())\n",
    "\n",
    "train_data, test_data = train_test_split(total_df, test_size = 0.25, random_state = 42) # 훈련용 테스트 데이터 나누기\n",
    "print('데이터 총 개수 :',len(total_df))\n",
    "print('훈련용 데이터 개수 :', len(train_data))\n",
    "print('테스트용 데이터 개수 :', len(test_data))\n",
    "print('----------------------------------------------------')\n",
    "\n",
    "plt.subplot(121)\n",
    "train_data['label'].value_counts().plot(kind = 'bar', title='Count')\n",
    "print(train_data.groupby('label').size().reset_index(name = 'count'))\n",
    "print('----------------------------------------------------')\n",
    "\n",
    "\n",
    "# 한글과 공백을 제외하고 모두 제거\n",
    "pd.set_option('mode.chained_assignment', None) # 경고 메시지 끄기\n",
    "train_data['text'] = train_data['text'].str.replace(\"#@(.+?)#|[^가-힣 ]\",\"\",regex=True)\n",
    "train_data.replace('', np.nan, inplace=True)\n",
    "#print(train_data.isnull().sum()) # null 데이터 확인\n",
    "test_data['text'] = test_data['text'].str.replace(\"#@(.+?)#|[^가-힣 ]\",\"\",regex=True)\n",
    "test_data.replace('', np.nan, inplace=True)\n",
    "#print(test_data.isnull().sum())\n",
    "\n",
    "# 한글은 형태소 분석기 사용해야됨 KoNPLY\n",
    "okt = Okt()\n",
    "stopwords = ['하다','이','가','에','는', '없다'] # spam, ham 빈도 수 높은 중복단어 제거\n",
    "\n",
    "X_train = []\n",
    "for sentence in train_data['text']:\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 처리\n",
    "    X_train.append(temp_X)\n",
    "\n",
    "X_test = []\n",
    "for sentence in test_data['text']:\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 처리\n",
    "    X_test.append(temp_X)\n",
    "    \n",
    "    \n",
    "train_data['tokenized'] = X_train\n",
    "test_data['tokenized'] = X_test\n",
    "\n",
    "spam_words = np.hstack(train_data[train_data.label == 1]['tokenized'].values)\n",
    "ham_words = np.hstack(train_data[train_data.label == 0]['tokenized'].values)\n",
    "\n",
    "spam_word_count = Counter(spam_words)\n",
    "print('스팸 단어 빈도')\n",
    "print(spam_word_count.most_common(20))\n",
    "print('----------------------------------------------------')\n",
    "\n",
    "ham_word_count = Counter(ham_words)\n",
    "print('일상 단어 빈도')\n",
    "print(ham_word_count.most_common(20))\n",
    "print('----------------------------------------------------')\n",
    "\n",
    "X_train = train_data['tokenized'].values\n",
    "y_train = train_data['label'].values\n",
    "X_test = test_data['tokenized'].values\n",
    "y_test = test_data['label'].values\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "threshold = 2 # 등장 횟수가 threshold인 단어는 제거\n",
    "word_to_index = tokenizer.word_index\n",
    "total_cnt = len(word_to_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "        \n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n",
    "\n",
    "#vocab_size = total_cnt - rare_cnt + 1\n",
    "#vocab_size = total_cnt + 1 # predict Error 해결\n",
    "\n",
    "vocab_size = total_cnt - rare_cnt + 2\n",
    "tokenizer = Tokenizer(vocab_size, oov_token = 'OOV')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "with open(file_name+\"_tokenizer.pickle\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# 빈 샘플들을 제거\n",
    "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning) # 경고 메시지 끄기\n",
    "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]\n",
    "print(\"빈 샘플 개수 :\",len(drop_train))\n",
    "X_train = np.delete(X_train, drop_train, axis=0)\n",
    "y_train = np.delete(y_train, drop_train, axis=0)\n",
    "\n",
    "print('최대 길이 :',max(len(l) for l in X_train))\n",
    "print('평균 길이 :',sum(map(len, X_train))/len(X_train))\n",
    "plt.subplot(122)\n",
    "plt.hist([len(s) for s in X_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "def below_threshold_len(max_len, nested_list):\n",
    "    cnt = 0\n",
    "    for s in nested_list:\n",
    "        if(len(s) <= max_len):\n",
    "            cnt = cnt + 1\n",
    "    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))\n",
    "\n",
    "max_len = 60 # 최대 길이 (그래프를 보고 판단)\n",
    "below_threshold_len(max_len, X_train)\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen = max_len)\n",
    "X_test = pad_sequences(X_test, maxlen = max_len)\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, max_len))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint(file_name+'.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=256, validation_split=0.2)\n",
    "\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))\n",
    "\n",
    "epochs = range(1, len(history.history['acc']) + 1)\n",
    "plt.plot(epochs, history.history['loss'])\n",
    "plt.plot(epochs, history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "# 변수 파일 저장\n",
    "d = {'max_data':max_data, 'min_data':min_data, 'max_len':max_len}\n",
    "json_var = json.dumps(d)\n",
    "with open(file_name+'_variable.json', 'w') as f:\n",
    "    f.write(json_var)\n",
    "\n",
    "# 불용어 단어 저장\n",
    "json_stopwords = json.dumps(stopwords)\n",
    "with open(file_name+'_stopwords.json', 'w') as f:\n",
    "    f.write(json_stopwords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
