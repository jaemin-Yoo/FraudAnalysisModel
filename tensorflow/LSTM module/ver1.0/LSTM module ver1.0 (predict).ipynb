{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 100)         2420100   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,537,477\n",
      "Trainable params: 2,537,477\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[[1687, 178, 4425, 661]]\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0 1687  178\n",
      "  4425  661]]\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000215D9476A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      " 83.81% 확률로 스팸입니다.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# wordInext 목록 받아오기\n",
    "with open('LSTM_module_ver1.0_wordIndex.json') as f:\n",
    "    word_index = json.load(f)\n",
    "    tokenizer.word_index = word_index\n",
    "\n",
    "model = tf.keras.models.load_model('LSTM_module_ver1.0.h5')\n",
    "model.summary()\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "def preTreatment(data):\n",
    "    stopwords = ['\\n','.','?',',','']\n",
    "    temp = okt.morphs(data, stem=True) # 토큰화\n",
    "    temp = [word for word in temp if not word in stopwords] # 불용어 처리\n",
    "    return temp\n",
    "\n",
    "\n",
    "def sentiment_predict(new_sentence):\n",
    "    data = tokenizer.texts_to_sequences([new_sentence]) # 단어를 숫자값, 인덱스로 변환하여 저장\n",
    "    print(data)\n",
    "    max_len = 100 # 전체 데이터 셋 길이 설정 (메일의 최대 길이)\n",
    "    pad_new = pad_sequences(data, maxlen = max_len) # 모든 메일의 길이를 100로 설정 (빈 부분은 0으로 패딩)\n",
    "    print(pad_new)\n",
    "    score = float(model.predict(pad_new))\n",
    "    if (score > 0.5):\n",
    "        print(' {:.2f}% 확률로 스팸입니다.'.format(score * 100))\n",
    "    else:\n",
    "        print(' {:.2f}% 확률로 스팸이 아닙니다.'.format((1 - score) * 100))\n",
    "\n",
    "txt = ''\n",
    "data = preTreatment(txt)\n",
    "sentiment_predict(data)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
