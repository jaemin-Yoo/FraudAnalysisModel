{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 100)         2420100   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,537,477\n",
      "Trainable params: 2,537,477\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "오빠 늦는다고 당당하다 너 몇 시에 올 건데 엄마 말 좀 잘 해 줘 그냥 근데 그게 네가 뭐 어떤게 뭐 있다고 염병을 하고 있네 야 근데 남자가 잘생겼어 키도 커 돈도 많아 근데이 사람이 불고기 화재 할 거야 안 할 거야 안 해 안 한다고 안 한다고 하는 비 올 수 있\n",
      "[[139, 252, 3493, 33, 246, 235, 6, 35, 871, 76, 45, 59, 36, 145, 101, 71, 29, 232, 39, 5, 42, 144, 42, 18, 9552, 28, 1, 18, 38, 29, 358, 5, 795, 3391, 199, 137, 10, 121, 554, 2493, 67, 3, 3504, 1, 1334, 12, 1, 1334, 12, 145, 12, 1, 12, 1, 1, 593, 35, 191, 18]]\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0  139\n",
      "   252 3493   33  246  235    6   35  871   76   45   59   36  145  101\n",
      "    71   29  232   39    5   42  144   42   18 9552   28    1   18   38\n",
      "    29  358    5  795 3391  199  137   10  121  554 2493   67    3 3504\n",
      "     1 1334   12    1 1334   12  145   12    1   12    1    1  593   35\n",
      "   191   18]]\n",
      " 99.97% 확률로 스팸이 아닙니다.\n",
      "겠지\n",
      "[[657]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0 657]]\n",
      " 56.85% 확률로 스팸입니다.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "import json\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# wordInext 목록 받아오기\n",
    "with open('LSTM_module_ver1.0_wordIndex.json') as f:\n",
    "    word_index = json.load(f)\n",
    "    tokenizer.word_index = word_index\n",
    "\n",
    "model = tf.keras.models.load_model('LSTM_module_ver1.0.h5')\n",
    "model.summary()\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "def preTreatment(data):\n",
    "    stopwords = ['\\n','.','?',',','']\n",
    "    temp = okt.morphs(data, stem=True) # 토큰화\n",
    "    temp = [word for word in temp if not word in stopwords] # 불용어 처리\n",
    "    return temp\n",
    "\n",
    "\n",
    "def sentiment_predict(new_sentence):\n",
    "    data = tokenizer.texts_to_sequences([new_sentence]) # 단어를 숫자값, 인덱스로 변환하여 저장\n",
    "    print(data)\n",
    "    max_len = 100 # 전체 데이터 셋 길이 설정 (메일의 최대 길이)\n",
    "    pad_new = pad_sequences(data, maxlen = max_len) # 모든 메일의 길이를 100로 설정 (빈 부분은 0으로 패딩)\n",
    "    print(pad_new)\n",
    "    score = float(model.predict(pad_new))\n",
    "    if (score > 0.5):\n",
    "        print(' {:.2f}% 확률로 스팸입니다.'.format(score * 100))\n",
    "    else:\n",
    "        print(' {:.2f}% 확률로 스팸이 아닙니다.'.format((1 - score) * 100))\n",
    "        \n",
    "        \n",
    "import speech_recognition as sr\n",
    "\n",
    "r = sr.Recognizer()\n",
    "harvard = sr.AudioFile('ham.wav') # 10MB 용량 제한\n",
    "with harvard as source:\n",
    "    audio = r.record(source)\n",
    "\n",
    "t = r.recognize_google(audio, language='ko-KR')\n",
    "x = [t[i:i+150] for i in range(0, len(t), 150)] # 텍스트 파일 중 150 길이로 데이터 길이 제한\n",
    "for txt in x:\n",
    "    print(txt)\n",
    "    data = preTreatment(txt)\n",
    "    sentiment_predict(data)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
