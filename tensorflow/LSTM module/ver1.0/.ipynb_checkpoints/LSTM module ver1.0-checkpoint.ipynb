{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "this version of pandas is incompatible with numpy < 1.13.3\nyour numpy version is 1.13.1.\nPlease upgrade numpy to >= 1.13.3 to use this pandas version",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-26ed7b8dc41d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# numpy compat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m from pandas.compat.numpy import (\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0m_np_version_under1p14\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0m_np_version_under1p15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\compat\\numpy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;34m\"your numpy version is {0}.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;34m\"Please upgrade numpy to >= 1.13.3 to use \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[1;34m\"this pandas version\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_np_version\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     )\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: this version of pandas is incompatible with numpy < 1.13.3\nyour numpy version is 1.13.1.\nPlease upgrade numpy to >= 1.13.3 to use this pandas version"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pandas import Series\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "temp_list = []\n",
    "train_X = []\n",
    "test_X = []\n",
    "train_y = []\n",
    "test_y = []\n",
    "train_cnt = 8000\n",
    "test_cnt = 2000\n",
    "\n",
    "# 피싱 대화 데이터 가져오기\n",
    "path = 'fraudDataset/spamData/'\n",
    "path1 = os.listdir(path)\n",
    "for a in path1:\n",
    "    path2 = path+a\n",
    "    with open(path2, 'r', encoding='utf-8') as f:\n",
    "        contents = f.read()\n",
    "        temp_list.append(contents)\n",
    "        \n",
    "for t in temp_list:\n",
    "    x = [t[i:i+150] for i in range(0, len(t), 150)] # 텍스트 파일 중 150 길이로 데이터 길이 제한\n",
    "    train_X.extend(x)\n",
    "\n",
    "# train, test Data 분리\n",
    "n_X = int(len(train_X) * 0.8)\n",
    "test_X = train_X[n_X:]\n",
    "train_X = train_X[:n_X]\n",
    "\n",
    "for y in range(len(train_X)):\n",
    "    train_y.append(1) # label 추가\n",
    "    \n",
    "for y in range(len(test_X)):\n",
    "    test_y.append(1)\n",
    "\n",
    "# 일상 대화 데이터 가져오기\n",
    "path = 'fraudDataset/hamData/'\n",
    "path1 = os.listdir(path)\n",
    "for a in path1:\n",
    "    path2 = os.listdir(path+a)\n",
    "    for b in path2:\n",
    "        path3 = path+a+'/'+b\n",
    "        with open(path3, 'r', encoding='utf-8') as f:\n",
    "            contents = f.read()\n",
    "            json_data = json.loads(contents)\n",
    "            data = ''\n",
    "            data = json_data['data']\n",
    "            for i in range(len(data)):                \n",
    "                sentence = ''\n",
    "                dialogue = data[i]['body']['dialogue']\n",
    "                for j in range(len(dialogue)):\n",
    "                    utterance = dialogue[j]['utterance']\n",
    "                    sentence += utterance+' '\n",
    "                \n",
    "                if a == 'Training':\n",
    "                    if train_y.count(0) == train_cnt:\n",
    "                        break\n",
    "                    train_X.append(sentence)\n",
    "                    train_y.append(0)\n",
    "                else:\n",
    "                    if test_y.count(0) == test_cnt:\n",
    "                        break\n",
    "                    test_X.append(sentence)\n",
    "                    test_y.append(0)\n",
    "\n",
    "print('훈련 데이터 총 개수:',len(train_X))\n",
    "print('테스트 데이터 총 개수:',len(test_X))\n",
    "print('훈련 데이터 스팸 개수',train_y.count(1))\n",
    "print('테스트 데이터 스팸 개수',test_y.count(1))\n",
    "print('훈련 데이터 일반 개수',train_y.count(0))\n",
    "print('테스트 데이터 일반 개수',test_y.count(0))\n",
    "print('--------------------------------')\n",
    "print('스팸 데이터 예시')\n",
    "print('\\n')\n",
    "print(train_X[0])\n",
    "print('--------------------------------')\n",
    "print('일반 데이터 예시')\n",
    "print('\\n')\n",
    "print(train_X[train_y.count(1)])\n",
    "print('--------------------------------')\n",
    "\n",
    "y_train = np.array(train_y)\n",
    "y_test = np.array(test_y)\n",
    "\n",
    "okt = Okt()\n",
    "stopwords = ['\\n','.','?',',','']\n",
    "\n",
    "X_train = []\n",
    "for sentence in train_X:\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 처리\n",
    "    X_train.append(temp_X)\n",
    "\n",
    "X_test = []\n",
    "for sentence in test_X:\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 처리\n",
    "    X_test.append(temp_X)\n",
    "    \n",
    "print('전처리 후 스팸 데이터 예시')\n",
    "print('\\n')\n",
    "print(X_train[0])\n",
    "print('--------------------------------')\n",
    "print('전처리 후 일반 데이터 예시')\n",
    "print('\\n')\n",
    "print(X_train[train_y.count(1)])\n",
    "print('--------------------------------')\n",
    "\n",
    "# 한글은 형태소 분석기 사용해야됨 KoNPLY\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "threshold = 3\n",
    "word_to_index = tokenizer.word_index\n",
    "total_cnt = len(word_to_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 토큰화된 각 단어 index를 json 형태로 저장\n",
    "import json\n",
    "json = json.dumps(word_to_index)\n",
    "with open(\"LSTM_module_ver1.0_wordIndex.json\", \"w\") as f:\n",
    "    f.write(json)\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "        \n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n",
    "\n",
    "#vocab_size = total_cnt - rare_cnt + 1\n",
    "vocab_size = total_cnt + 1 # predict Error 해결\n",
    "tokenizer = Tokenizer(vocab_size) \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# 빈 샘플들을 제거\n",
    "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]\n",
    "X_train = np.delete(X_train, drop_train, axis=0)\n",
    "y_train = np.delete(y_train, drop_train, axis=0)\n",
    "print('최대 길이 :',max(len(l) for l in X_train))\n",
    "print('평균 길이 :',sum(map(len, X_train))/len(X_train))\n",
    "plt.hist([len(s) for s in X_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "max_len = 100 # 최대 길이\n",
    "X_train = pad_sequences(X_train, maxlen = max_len)\n",
    "X_test = pad_sequences(X_test, maxlen = max_len)\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, max_len))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('LSTM_module_ver1.0.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)\n",
    "\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))\n",
    "\n",
    "epochs = range(1, len(history.history['acc']) + 1)\n",
    "plt.plot(epochs, history.history['loss'])\n",
    "plt.plot(epochs, history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
