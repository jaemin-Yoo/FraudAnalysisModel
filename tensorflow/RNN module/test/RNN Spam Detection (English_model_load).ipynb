{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 32)          285472    \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 287,585\n",
      "Trainable params: 287,585\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[[781, 78, 4, 1017, 454, 259, 3, 16, 108, 489, 2, 4024, 1367, 154, 936, 2, 134, 17, 4025, 134, 405, 4026, 521, 937, 605, 64]]\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001EC7AFDCAE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      " 98.83% 확률로 spam 입니다.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# wordInext 목록 받아오기\n",
    "with open('wordIndex.json') as f:\n",
    "    word_index = json.load(f)\n",
    "    tokenizer.word_index = word_index\n",
    "\n",
    "model = tf.keras.models.load_model('RNN_model.h5')\n",
    "model.summary()\n",
    "\n",
    "txt = \"WINNER!! As a valued network customer you have been selected to receivea å£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\"\n",
    "\n",
    "def sentiment_predict(new_sentence):\n",
    "    data = tokenizer.texts_to_sequences([new_sentence]) # 단어를 숫자값, 인덱스로 변환하여 저장\n",
    "    print(data)\n",
    "    max_len = 189 # 전체 데이터 셋 길이 설정 (메일의 최대 길이)\n",
    "    pad_new = pad_sequences(data, maxlen = max_len) # 모든 메일의 길이를 189로 설정 (빈 부분은 0으로 패딩)\n",
    "    score = float(model.predict(pad_new))\n",
    "    if (score > 0.5):\n",
    "        print(' {:.2f}% 확률로 spam 입니다.'.format(score * 100))\n",
    "    else:\n",
    "        print(' {:.2f}% 확률로 spam이 아닙니다.'.format((1 - score) * 100))\n",
    "        \n",
    "sentiment_predict(txt)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
