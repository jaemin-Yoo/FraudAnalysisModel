{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "this version of pandas is incompatible with numpy < 1.13.3\nyour numpy version is 1.13.1.\nPlease upgrade numpy to >= 1.13.3 to use this pandas version",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e21b28ecb916>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# numpy compat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m from pandas.compat.numpy import (\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0m_np_version_under1p14\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0m_np_version_under1p15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\compat\\numpy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;34m\"your numpy version is {0}.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;34m\"Please upgrade numpy to >= 1.13.3 to use \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[1;34m\"this pandas version\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_np_version\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     )\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: this version of pandas is incompatible with numpy < 1.13.3\nyour numpy version is 1.13.1.\nPlease upgrade numpy to >= 1.13.3 to use this pandas version"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from pandas import Series\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "temp_list = []\n",
    "x_list = []\n",
    "y_list = []\n",
    "min = 99999\n",
    "n = 32\n",
    "\n",
    "# Read spam Data\n",
    "for i in range(n):\n",
    "    path = 'textfile/'+str(i)+'.txt'\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read() # 파일 읽어오기\n",
    "        textlen = len(text) # 파일 내용 길이\n",
    "        if textlen < 200: # 200자 미만이면 데이터로 취급안함\n",
    "            continue\n",
    "            \n",
    "        if textlen < min:\n",
    "            min = textlen\n",
    "            \n",
    "        temp_list.append(text)\n",
    "\n",
    "print('최소 길이 :', min)\n",
    "        \n",
    "for t in temp_list:\n",
    "    x = [t[i:i+min] for i in range(0, len(t), min)] # 텍스트 파일 중 최소 길이로 데이터 길이 제한\n",
    "    x_list.extend(x)\n",
    "\n",
    "for y in range(len(x_list)):\n",
    "    y_list.append(1) # label 추가\n",
    "\n",
    "print('데이터 개수 :',len(x_list))\n",
    "\n",
    "# Read ham Data\n",
    "with open('textfile/ham_daily.txt', 'r', encoding='utf-8') as f:\n",
    "    data = ''\n",
    "    cnt = 1\n",
    "    loop = 1\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        \n",
    "        if line == '': # 다 읽으면 나오기\n",
    "            break\n",
    "        \n",
    "        data += line\n",
    "        \n",
    "        if cnt == 2: # 데이터가 질문, 대답 형식이라 2개 라인씩 짝짓기\n",
    "            data = re.sub('[0-9]|\\t|\\.', '', data) # 숫자, 탭, 특수문자 제거\n",
    "            x_list.append(data)\n",
    "            y_list.append(0) # ham 레이블 0\n",
    "            data = ''\n",
    "            cnt = 1\n",
    "            loop += 1\n",
    "            \n",
    "        # n개 데이터만 저장하기\n",
    "        if loop > n:\n",
    "            break\n",
    "            \n",
    "        cnt += 1\n",
    "        \n",
    "X_data = Series(x_list)\n",
    "y_data = Series(y_list)\n",
    "\n",
    "print(y_data.value_counts()) # 각 label별 데이터 갯수\n",
    "\n",
    "okt = Okt()\n",
    "stopwords = []\n",
    "X_train = []\n",
    "for sentence in X_data:\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 처리\n",
    "    X_train.append(temp_X)\n",
    "\n",
    "\n",
    "# 한글은 형태소 분석기 사용해야됨 KoNPLY\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train) # 5169개의 행을 가진 x의 각 행에 토큰화를 수행\n",
    "sequences = tokenizer.texts_to_sequences(X_train) # 단어를 숫자값, 인덱스로 변환하여 저장\n",
    "threshold = 2\n",
    "word_to_index = tokenizer.word_index\n",
    "total_cnt = len(word_to_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 토큰화된 각 단어 index를 json 형태로 저장\n",
    "import json\n",
    "json = json.dumps(word_to_index)\n",
    "with open(\"ver1.0_wordIndex.json\", \"w\") as f:\n",
    "    f.write(json)\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "    \n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if (value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "        \n",
    "print('등장 빈도가 %s 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어의 비율:', (rare_cnt / total_cnt)*100)\n",
    "print('전체 등장 빈도에서 희귀 단어 등장 빈도 비율:', (rare_freq / total_freq)*100)\n",
    "\n",
    "vocab_size = len(word_to_index) + 1\n",
    "print('단어 집합의 크기: {}'.format((vocab_size)))\n",
    "\n",
    "n_of_train = int(len(sequences) * 0.8)\n",
    "n_of_test = int(len(sequences) - n_of_train) # 훈련 : 테스트 = 8 : 2\n",
    "print('훈련 데이터 개수 :', n_of_train)\n",
    "print('테스트 데이터 개수 :', n_of_test)\n",
    "\n",
    "X_data = sequences\n",
    "print('메일의 최대 길이 : %d' % max(len(l) for l in X_data))\n",
    "print('메일의 평균 길이 : %f' % (sum(map(len, X_data))/len(X_data)))\n",
    "plt.hist([len(s) for s in X_data], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "max_len = 2747 # 전체 데이터 셋 길이 설정 (메일의 최대 길이)\n",
    "data = pad_sequences(X_data, maxlen = max_len) # 모든 메일의 길이를 메일의 최대 길이로 설정 (빈 부분은 0으로 패딩)\n",
    "print('훈련 데이터의 크기(shape): ', data.shape)\n",
    "\n",
    "X_test = data[n_of_train:]\n",
    "y_test = np.array(y_data[n_of_train:])\n",
    "X_train = data[:n_of_train]\n",
    "y_train = np.array(y_data[:n_of_train]) # 훈련, 테스트 데이터 분류\n",
    "\n",
    "from tensorflow.keras.layers import SimpleRNN, Embedding ,Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 32)) # 32 차원 벡터\n",
    "model.add(SimpleRNN(32)) # RNN 셀의 hidden_size는 32\n",
    "model.add(Dense(1, activation='sigmoid')) # 이진 분류이므로 시그모이드 함수 사용\n",
    "\n",
    "#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4) # 과적합 방지로 검증 데이터 손실이 4회 증가하면 학습 조기 종료\n",
    "# mc = ModelCheckpoint('RNN module ver1.0.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True) # 검증 데이터의 정확도가 이전보다 좋아질 경우에만 모델 저장\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.2) # 훈련 데이터의 20%를 검증데이터로 사용\n",
    "# history = model.fit(X_train, y_train, epochs=4, callbacks=[es, mc], batch_size=64, validation_split=0.2) # 훈련 데이터의 20%를 검증데이터로 사용\n",
    "\n",
    "print('\\n 테스트 정확도: %.4f' % (model.evaluate(X_test, y_test)[1]))\n",
    "\n",
    "epochs = range(1, len(history.history['acc']) + 1)\n",
    "plt.plot(epochs, history.history['loss'])\n",
    "plt.plot(epochs, history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "model.save('RNN module ver1.0.h5')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
