{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10672/2151256268.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOkt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwordcloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "okt = Okt()\n",
    "'''\n",
    "txt = \"도움 되셨다면 공감을 꾸욱 눌러주세요. 안녕하세요 저는 유재민이라고 합니다. gggg ㅎㅎ\"\n",
    "print(okt.morphs(txt)) # 형태소 단위로 구문 분석 수행\n",
    "print(okt.nouns(txt)) # 명사만 추출 (띄어쓰기 안해도 추출이 잘됨)\n",
    "print(okt.phrases(txt)) # 어절만 추출\n",
    "print(okt.pos(txt)) # 형태소 단위로 쪼갠 후 각 품사들을 태깅해서 리스트 형태로 반환 (영어단어-Alpha, ^^or?-Puntuation, ㅋㅋㅋ-KoreanParticle)\n",
    "'''\n",
    "f = open('1.txt','r',encoding='utf-8')\n",
    "text = f.read() # 파일 읽기\n",
    "\n",
    "noun = okt.nouns(text) # 명사만 추출\n",
    "for v in noun[:]:\n",
    "    if len(v)<2:\n",
    "        noun.remove(v) # 한글자 명사 제거\n",
    "\n",
    "# 불용어 처리\n",
    "result = []\n",
    "stop_words = \"저희,지금,그거,네네\"\n",
    "stop_word = stop_words.split(',')\n",
    "for n in noun:\n",
    "    if n not in stop_words:\n",
    "        result.append(n)\n",
    "\n",
    "count = Counter(result) # 각 단어 별 count 설정\n",
    "\n",
    "noun_list = count.most_common(10) # 빈도가 높은 n개의 명사 추출\n",
    "\n",
    "for v in noun_list:\n",
    "    print(v)\n",
    "\n",
    "noun_text = ''\n",
    "for n in result:\n",
    "    noun_text = noun_text+' '+n\n",
    "\n",
    "wordcloud = WordCloud(font_path='MaruBuri-Regular.ttf', background_color='white').generate(noun_text) # wordcloud 생성\n",
    "\n",
    "plt.figure(figsize=(22,22)) #이미지 사이즈 지정\n",
    "plt.imshow(wordcloud, interpolation='lanczos') #이미지의 부드럽기 정도\n",
    "plt.axis('off') #x y 축 숫자 제거\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
